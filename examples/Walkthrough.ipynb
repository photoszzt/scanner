{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scanner walkthrough\n",
    "\n",
    "To explore how Scanner works, we're going to walk through a simple video analysis application. If you want to analyze a film, a common unit of analysis is the _shot_, short segments of video often delineated by the camera cutting to a different angle or location. In this walkthrough, we're going to use Scanner to implement _shot segmentation_, or breaking up a video into shots. To start, we need to get a video. We'll use a scene from Moana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/79DijItQXMM\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We've set up some scripts to help you download the video in the snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "path = util.download_video()\n",
    "print path\n",
    "\n",
    "# Read all the frames\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from timeit import default_timer as now\n",
    "\n",
    "start = now()\n",
    "video = cv2.VideoCapture(path)\n",
    "frames = []\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret: break\n",
    "    frames.append(frame)\n",
    "video.release()\n",
    "print 'Time to read frames: {:.3f}s'.format(now() - start)\n",
    "\n",
    "# Display the tenth frame    \n",
    "plt.imshow(cv2.cvtColor(frames[10], cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take another look at the video and see if you can identify when shots change. Our shot segmentation algorithm uses the following intuition: in a video, most frames are similar to the one following it. Because most shot changes happen with cuts (as opposed to dissolves or fades), there's an immediate visual break from one frame to the next. We want to identify when the change in visual content between two adjacent frames is substantially larger than normal. One way to estimate change in visual content is by computing a histogram of colors for each frame, i.e. count the number of dark pixels and light pixels in each color channel (red/green/blue), and then compute the magnitude of difference between adjacent frames' histograms. Let's visualize this for the above video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "histograms = []\n",
    "N = len(frames)\n",
    "\n",
    "# Compute 3 color histograms (one for each channel) for each video frame\n",
    "start = now()\n",
    "for frame in frames:\n",
    "    hists = [cv2.calcHist([frame], [channel], None, [16], [0, 256]) \n",
    "             for channel in range(3)]\n",
    "    histograms.append(hists)\n",
    "print 'Time to compute histograms: {:.3f}s'.format(now() - start)\n",
    "\n",
    "# Compute differences between adjacent pairs of histograms\n",
    "def compute_histogram_diffs(histograms):    \n",
    "    diffs = []        \n",
    "    for i in range(1, N):\n",
    "        frame_diffs = [distance.chebyshev(histograms[i-1][channel], histograms[i][channel]) \n",
    "                       for channel in range(3)]\n",
    "        avg_diff = np.mean(frame_diffs)\n",
    "        diffs.append(avg_diff)\n",
    "    return diffs\n",
    "        \n",
    "diffs = compute_histogram_diffs(histograms)\n",
    "\n",
    "# Plot the differences\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]\n",
    "plt.xlabel(\"Frame number\")\n",
    "plt.ylabel(\"Difference from previous frame\")\n",
    "plt.plot(range(1, N), diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows, for each frame, the difference between its color histograms and the previous frame's color histograms. Try playing around with the number of histogram bins as well as the [distance metric](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html). As you can see, there are a number of sharp peaks interspersed throughout the video that likely correspond to shot boundaries. We can run a sliding window over the above graph to find the peaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 500  # The size of our sliding window (how many data points to include)\n",
    "OUTLIER_STDDEV = 3 # Outliers are N standard deviations away from the mean of the sliding window\n",
    "\n",
    "def find_shot_boundaries(diffs):\n",
    "    boundaries = []\n",
    "    for i in range(1, N):\n",
    "        window = diffs[max(i-WINDOW_SIZE,0):min(i+WINDOW_SIZE,N)]\n",
    "        if diffs[i-1] - np.mean(window) > OUTLIER_STDDEV * np.std(window):\n",
    "            boundaries.append(i)\n",
    "    return boundaries\n",
    "\n",
    "boundaries = find_shot_boundaries(diffs)        \n",
    "\n",
    "print 'Shot boundaries are:'\n",
    "print boundaries\n",
    "\n",
    "from scannerpy.stdlib.montage import make_montage\n",
    "montage = make_montage(len(boundaries), iter([frames[i] for i in boundaries]))\n",
    "plt.imshow(cv2.cvtColor(montage, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we've done it! The video is now segmented in shots. At this point, you're probably wondering: \"...but I thought this was a Scanner tutorial!\" Well, consider now: what if you wanted to run this pipeline over a second trailer? A movie? A thousand movies? The simple Python code we wrote above is great for experimenting, but doesn't scale. To accelerate this analysis, we need to speed up the core computation, computing the color histogram. Here are some ways we can make that faster:\n",
    "\n",
    "* Use a faster histogram implementation, e.g. using the GPU.\n",
    "* Use a faster video decoder, e.g. the hardware decoder.\n",
    "* Parallelize the histogram pipeline on multiple CPUs or GPUs.\n",
    "* Parallelize the histogram pipeline across a cluster of machines.\n",
    "\n",
    "All of that is fairly difficult to do with Python, but easy with Scanner. \n",
    "\n",
    "Now I'm going to walk you through running the histogram computation in Scanner. First, we start by setting up our Scanner database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scannerpy import Database, DeviceType, Job, BulkJob\n",
    "\n",
    "db = Database()\n",
    "# If you have a cluster, you can specify: Database(master='localhost:5001', workers=['worker1:5002', 'worker2:5002'...])\n",
    "# By default, the database will only run on the local machine.\n",
    "[input_table], _ = db.ingest_videos([('example', path)], force=True)\n",
    "print db.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scanner, all data is organized into tables, just like a database. Videos are represented as tables where each frame is a row. You can see in the summary that the `example` table has two columns, `index` like the `id` field in a SQL database and `frame` which represents the raw RGB pixels. Scanner columns have one of two types: either it's a frame (i.e. image or video), or it's a byte array.\n",
    "\n",
    "To create a video table, you run `db.ingest_videos(list of (table name, path) pairs)`. Each table has a name specified the first element in the ingested pairs. By default, `ingest_videos` will raise a `ScannerException` if you attempt to ingest over an existing table, but `force=True` will allow such behavior. Next, we want to compute the histogram over each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = db.ops.FrameInput()\n",
    "histogram = db.ops.Histogram(\n",
    "    frame = frame,\n",
    "    device = DeviceType.GPU) # Change this to DeviceType.CPU if you don't have a GPU\n",
    "output = db.ops.Output(columns = [histogram])\n",
    "job = Job(op_args = {\n",
    "    frame: db.table('example').column('frame'),\n",
    "    output: 'example_hist'\n",
    "})\n",
    "bulk_job = BulkJob(output = output, jobs = [job])\n",
    "\n",
    "start = now()\n",
    "[output_table] = db.run(bulk_job, force=True) \n",
    "print 'Time to decode + compute histograms: {:.3f}'.format(now() - start)\n",
    "print db.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computations in Scanner are defined in a *data-parallel* manner--that is, you write a computation that takes in one (or a few) frames at a time, and then the Scanner runtime runs your computation in parallel across your video. Here, we define a computation that computes a color histogram for each frame in the video. This is done by defining a series of \"ops\" (operators, similar to TensorFlow):\n",
    "1. The `FrameInput` op represents a single frame, the input to our computation. This will be drawn from a video.\n",
    "2. `Histogram` is an op that computes a color histogram over the input `frame`. We specify that it should run on the CPU.\n",
    "3. `Output` represents the final output of our computation, the data that will get written back to our database, in this case a table with a single column containing the histogram for each frame of the input table.\n",
    "\n",
    "Once we define a computation, then a `Job` provides parameters to the computation, here saying which table the frames should be drawn from (`example`), and what the name of the output table should be (`example_hist`). Lastly, we define a `BulkJob` with containing our job and the output node (\"bulk\" because we can run many jobs in parallel), and then tell the database to run those jobs.\n",
    "\n",
    "After a job is complete, we want to load the results of our computation into Python for further processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scannerpy.stdlib import parsers\n",
    "from pprint import pprint\n",
    "rows = output_table.load(['histogram'], parsers.histograms)\n",
    "histograms = [h for _, h in rows]\n",
    "\n",
    "# Run the same shot detection pipeline as before\n",
    "diffs = compute_histogram_diffs(histograms)\n",
    "boundaries = find_shot_boundaries(diffs)\n",
    "montage = make_montage(len(boundaries), iter([frames[i] for i in boundaries]))\n",
    "plt.imshow(cv2.cvtColor(montage, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Scanner does not have a built-in type system, loading columns from a table requires a parsing function that converts between raw byte strings and Python-understandable data types. For example, the `parsers.histograms` parser function returns a nested Numpy array of the three histograms. Once you've parsed the table column, that's it! You now have your Scanner computation results loaded and ready to go.\n",
    "\n",
    "Let's reflect for a moment on the script we just made. Is it any faster than before? Going back to our four bullet points:\n",
    "\n",
    "* Scanner will run your computation on the GPU (`device=DeviceType.GPU`).\n",
    "* Scanner will use accelerated hardware video decode behind the scenes.\n",
    "* Scanner will automatically run on all of your CPU cores and on multiple GPUs.\n",
    "* Scanner will automatically distribute the work across a cluster.\n",
    "\n",
    "That's what you get for free using Scanner for your video analyses. All of the code for organizing, distributing, and decoding your videos is taken care of by the Scanner runtime. As an exercise, download a long video like a movie and try running both our Python histogram pipeline and the Scanner pipeline. You'll likely notice a substantial difference!\n",
    "\n",
    "So, where should you go from here? I would check out:\n",
    "* [Extended tutorial](https://github.com/scanner-research/scanner/tree/master/examples/tutorial): covers more Scanner features like sampling patterns and building custom ops.\n",
    "* [Code examples](https://github.com/scanner-research/scanner/tree/master/examples): other applications like face detection and reverse image search implemented with Scanner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
